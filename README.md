# ğŸ§  Large Language Models Bootcamp
## #90DaysLLMs [17th June - 25th Aug 2024]

A 90-day hands-on bootcamp designed to equip participants with practical skills in developing, deploying, and scaling applications powered by Large Language Models (LLMs). The program blends theory with real-world applications through capstone projects, live sessions, and tools like Hugging Face, CrewAI, LangChain, and more.

---

## ğŸ“š Curriculum Overview

### ğŸ”° Practical Introduction to LLMs
- Applied LLM Foundations
- Real-World LLM Use Cases
- Domain and Task Adaptation Techniques

### ğŸ’¡ Prompting and Prompt Engineering
- Basic Prompting Principles
- Prompting Types (Zero-shot, Few-shot, Chain-of-Thought, etc.)
- Applications, Risks, and Advanced Prompting Techniques

### ğŸ”§ LLM Fine-Tuning
- Fine-Tuning Basics
- Instruction vs. Domain-Specific Fine-Tuning
- Fine-Tuning Challenges (Overfitting, Data Quality, etc.)

### ğŸ“¦ Retrieval-Augmented Generation (RAG)
- Introduction to RAG
- Chunking, Embedding, and Vector Stores
- Advanced RAG Concepts: Ranking, Fusion, Guardrails, etc.

### ğŸ› ï¸ Tools for Building LLM Applications
- Fine-Tuning Libraries (Hugging Face Transformers, PEFT)
- RAG Frameworks (Haystack, LlamaIndex, LangChain)
- Tooling for Observability, Prompt Management, Vector Search, and Deployment

### ğŸ“Š Evaluation Techniques
- Behavior vs. Performance Evaluation
- Common Benchmarks (HELM, MMLU, etc.)
- Metrics: BLEU, ROUGE, Perplexity, Accuracy, etc.

### ğŸ§ª Build Your Own LLM App
- Define End-to-End LLM Workflow
- Build with Open Source Tools
- Deploy Using Free Resources (Gradio, Streamlit, Colab, etc.)

### ğŸš€ Advanced Features & Deployment
- LLM Lifecycle and LLMOps
- Scalable LLM Deployment (Docker, GitHub Actions)
- Monitoring, Ranking, and Explainability

### âš ï¸ LLM Challenges
- Scaling and Memory Optimization
- Behavioral Issues (Hallucination, Alignment)
- Security and Deployment Risks

### ğŸ”¬ Emerging Research Trends
- Multimodal LLMs (Vision, Audio, Text)
- Open Source Innovations
- AI Agents and Agentic Workflows
- Novel Architectures (MoE, Mamba, RWKV, etc.)

### ğŸ§  LLM Foundations
- Generative Models Overview
- RNNs, LSTMs, Seq2Seq Models
- Transformers and Self-Attention Mechanism

---

## ğŸ—“ Weekly Breakdown (Selected Highlights)

### ğŸ• Week 0: Base Camp
- Python Refresher, Environment Setup
- FastAPI + Streamlit for LLM UI Prototyping

### ğŸ“– Week 1â€“2: NLP + Hugging Face
- Data Handling, Tokenization
- Pretrained Model Fine-Tuning

### ğŸ’¬ Week 3â€“5: LLMs, Chatbots & RAG
- Conversational Bot with Groq + LLaMA
- Chunking, Embedding, and Search (FAISS/QDrant)

### ğŸ” Week 6â€“10: Multimodal + RAG with Tabular & Images
- RAG for Tables, Audio, Images, Video
- Summarization and Translation Tools

### ğŸ§  Week 11â€“16: Agentic AI with CrewAI, phiData, LangChain, Haystack, LlamaIndex
- Role-based Agents, Multi-Agent Workflows
- Custom Toolkits and Integration

### ğŸ—ï¸ Week 17â€“24: Capstone Project
- Develop, Deploy and Scale your AI App
- CI/CD, Monitoring, Evaluation, Presentation

---

## ğŸ’¼ Capstone Project

Each participant will:
- Design an end-to-end LLM-powered application.
- Integrate prompting, fine-tuning, RAG, and evaluation.
- Use modern frameworks and best practices.
- Present a production-ready AI solution at Demo Day.

---

## ğŸ§‘â€ğŸ’» Tools & Technologies

- Python, FastAPI, Streamlit, Gradio
- Hugging Face, Groq, CrewAI, LangChain, Haystack
- Docker, GitHub Actions, Jupyter, VS Code, Colab
- FAISS, QDrant, LanceDB

---

## ğŸ¤ Community & Support

- Weekly live sessions with engineers & mentors
- Dedicated TAs for doubt resolution
- Monthly guest sessions by industry leaders
- Active peer-learning community

---

## ğŸ“¬ Stay Updated

Follow the hashtag **#90DaysLLMs** on [LinkedIn](https://linkedin.com) and [Twitter](https://twitter.com) to see project highlights and learning milestones.

---

# ğŸ§  LLM-Crafter: Build Your Own Language Model

Welcome to LLM-Crafter â€” a step-by-step guide and implementation to build, train, and deploy a Language Model from scratch using PyTorch.

## ğŸš€ What Youâ€™ll Learn
- How tokenizers work
- How to prepare custom datasets
- How to implement GPT-style architectures
- Training strategies and optimization tricks
- Evaluating and fine-tuning LLMs
- Generating text and building inference pipelines

## ğŸ“‚ Project Structure
- `01_tokenizer/`: Tokenizer from scratch (BPE or WordPiece)
- `02_dataset/`: Prepare your text data
- `03_model/`: GPT-style transformer architecture
- `04_training/`: Training loop, logging, early stopping
- `05_evaluation/`: Perplexity, BLEU, accuracy metrics
- `06_inference/`: Prompt-based inference pipeline
- `07_experiments/`: Logs and comparisons

## ğŸ› ï¸ Setup

```bash
git clone https://github.com/yourusername/LLM-Crafter.git
cd LLM-Crafter
pip install -r requirements.txt

